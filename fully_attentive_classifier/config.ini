[Self_Attentive_Model]
classes = 2
vectors_cache = /home/ifajcik/PycharmProjects/Figurative_Language/.vector_cache

embedding_class = Embedder
embeddings = fasttext.en.300d
embedding_dim = 300
batch_size = 32
lr = 0.0002
cuda = true
epochs = 30
lr_decrease_factor = 0.7
optimize_embeddings = False
gradient_clipping_threshold = 0.5
penalization = 0.0

d_model = 300
heads = 1
N = 1

ATTENTION_dropout = 0.3
ATTENTION_nhidden = 350
ATTENTION_hops = 25

FC_nhidden = 500
OUTPUT_dropout = 0.3

RNN_dropout = 0.3
RNN_nhidden = 300
RNN_layers = 3
